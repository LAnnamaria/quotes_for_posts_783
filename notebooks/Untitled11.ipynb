{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#All installation needed for the model\n",
        "\n"
      ],
      "metadata": {
        "id": "bsGQIbP_SkKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yDqsm1FRNvgS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install TensorFlow\n",
        "!pip install Keras\n",
        "!pip install pillow\n",
        "!pip install NumPy\n",
        "!pip install tqdm\n",
        "!pip install time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading"
      ],
      "metadata": {
        "id": "ZPC6_HtFOLm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from time import time\n",
        "\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "j4kTEBiATOSd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse "
      ],
      "metadata": {
        "id": "0uxJ58tRTWiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11be50a-7aef-4c2b-962a-feb1f2cfaddd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0   130k      0 --:--:-- --:--:-- --:--:--  130k\n",
            "OK\n",
            "75 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "gcsfuse is already the newest version (0.40.0).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 75 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir raw_data\n",
        "!gcsfuse --implicit-dirs image_caption raw_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w83yLo51iidO",
        "outputId": "e4612aef-06be-47fa-e7a3-4a74eae52066"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘raw_data’: File exists\n",
            "2022/03/06 17:53:51.739675 Start gcsfuse/0.40.0 (Go version go1.17.6) for app \"\" using mount point: /content/raw_data/raw_data/raw_data\n",
            "2022/03/06 17:53:51.752394 Opening GCS connection...\n",
            "2022/03/06 17:53:52.422116 Mounting file system \"image_caption\"...\n",
            "2022/03/06 17:53:52.428436 File system has been successfully mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd raw_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBrdON8hjCgN",
        "outputId": "833db626-c09f-44f6-c183-02128de90508"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/raw_data/raw_data/raw_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_path = \"raw_data/Flickr8k_text/Flickr8k.token.txt\"\n",
        "train_images_path = 'raw_data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "test_images_path = 'raw_data/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "images_path = 'raw_data/Flicker8k_Dataset/'\n",
        "glove_path = 'raw_data/glove.6B.300d.txt'\n",
        "\n",
        "doc = open(token_path,'r').read()\n",
        "print(doc[:410])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkrcIJpRmd3w",
        "outputId": "6c16474e-969b-476e-ab3a-fbfd06d27056"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
            "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = dict()\n",
        "for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line) > 2:\n",
        "          image_id = tokens[0].split('.')[0]\n",
        "          image_desc = ' '.join(tokens[1:])\n",
        "          if image_id not in descriptions:\n",
        "              descriptions[image_id] = list()\n",
        "          descriptions[image_id].append(image_desc)"
      ],
      "metadata": {
        "id": "VU6bYcq-VWPd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = str.maketrans('', '', string.punctuation)\n",
        "for key, desc_list in descriptions.items():\n",
        "    for i in range(len(desc_list)):\n",
        "        desc = desc_list[i]\n",
        "        desc = desc.split()\n",
        "        desc = [word.lower() for word in desc]\n",
        "        desc = [w.translate(table) for w in desc]\n",
        "        desc_list[i] =  ' '.join(desc)"
      ],
      "metadata": {
        "id": "85DPjIyLVZpA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set()\n",
        "for key in descriptions.keys():\n",
        "        [vocabulary.update(d.split()) for d in descriptions[key]]\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIs5GSIVo_5F",
        "outputId": "8ddbc7a0-175e-464f-cfbd-f85da47a3ae7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Vocabulary Size: 8828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = list()\n",
        "for key, desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "        lines.append(key + ' ' + desc)\n",
        "new_descriptions = '\\n'.join(lines)"
      ],
      "metadata": {
        "id": "b-4aAfnupIWi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = open(train_images_path,'r').read()\n",
        "dataset = list()\n",
        "for line in doc.split('\\n'):\n",
        "    if len(line) > 1:\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "\n",
        "train = set(dataset)"
      ],
      "metadata": {
        "id": "Ilw_vAu9pLAC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = glob.glob(images_path + '*.jpg')\n",
        "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
        "train_img = []\n",
        "for i in img: \n",
        "    if i[len(images_path):] in train_images:\n",
        "        train_img.append(i)\n",
        "\n",
        "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
        "test_img = []\n",
        "for i in img: \n",
        "    if i[len(images_path):] in test_images: \n",
        "        test_img.append(i)"
      ],
      "metadata": {
        "id": "6LqSB1KdpNmy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_descriptions = dict()\n",
        "for line in new_descriptions.split('\\n'):\n",
        "    tokens = line.split()\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    if image_id in train:\n",
        "        if image_id not in train_descriptions:\n",
        "            train_descriptions[image_id] = list()\n",
        "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "        train_descriptions[image_id].append(desc)"
      ],
      "metadata": {
        "id": "A8V2PDO9pP5K"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)"
      ],
      "metadata": {
        "id": "B9V5xPJLpTln"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0xGhUuYpofw",
        "outputId": "f5202f3b-b4b8-462a-febb-5ce9e4876dcd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary = 1659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ],
      "metadata": {
        "id": "JAZyMqBSpZ0b"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_desc = list()\n",
        "for key in train_descriptions.keys():\n",
        "    [all_desc.append(d) for d in train_descriptions[key]]\n",
        "lines = all_desc\n",
        "max_length = max(len(d.split()) for d in lines)\n",
        "\n",
        "print('Description Length: %d' % max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78HPGtRJpgkD",
        "outputId": "91d7aea2-9311-41ad-d52b-d9b927f5283c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description Length: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {} \n",
        "f = open(os.path.join(glove_path), encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "Ixi4VioOpt-y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in wordtoix.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "rt8ON4eKQL2W"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionV3(weights='imagenet')\n",
        "model_new = Model(model.input, model.layers[-2].output)"
      ],
      "metadata": {
        "id": "xzNPVqdWpw1J"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path):\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "aBlSVCx9qojR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(image):\n",
        "    image = preprocess(image) \n",
        "    fea_vec = model_new.predict(image) \n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
        "    return fea_vec\n",
        "%%capture\n",
        "encoding_train = {}\n",
        "for img in train_img:\n",
        "    encoding_train[img[len(images_path):]] = encode(img)\n",
        "train_features = encoding_train\n",
        "\n",
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "    encoding_test[img[len(images_path):]] = encode(img)"
      ],
      "metadata": {
        "id": "WXVjVk38qrmi"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL12pY7zqvm9",
        "outputId": "c7da6a72-5348-4041-937d-62db72154a72"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 2048)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 38, 300)      498000      ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 2048)         0           ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 38, 300)      0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          524544      ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 256)          570368      ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 256)          0           ['dense_1[0][0]',                \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1660)         426620      ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,085,324\n",
            "Trainable params: 2,085,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "RNwroIKdq5uj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key+'.jpg']\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            if n==num_photos_per_batch:\n",
        "                yield ([array(X1), array(X2)], array(y))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "metadata": {
        "id": "_YN0eF4HrKkJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 3\n",
        "steps = len(train_descriptions)//batch_size\n",
        "\n",
        "generator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\n",
        "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev0rBbprrORx",
        "outputId": "1e83e160-0308-4822-b265-6b4c83d6ac65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2000/2000 [==============================] - 227s 111ms/step - loss: 3.6189\n",
            "Epoch 2/30\n",
            "2000/2000 [==============================] - 223s 111ms/step - loss: 2.9669\n",
            "Epoch 3/30\n",
            "2000/2000 [==============================] - 217s 109ms/step - loss: 2.7780\n",
            "Epoch 4/30\n",
            "2000/2000 [==============================] - 215s 108ms/step - loss: 2.6618\n",
            "Epoch 5/30\n",
            "2000/2000 [==============================] - 214s 107ms/step - loss: 2.5773\n",
            "Epoch 6/30\n",
            "2000/2000 [==============================] - 214s 107ms/step - loss: 2.5109\n",
            "Epoch 7/30\n",
            "2000/2000 [==============================] - 213s 107ms/step - loss: 2.4595\n",
            "Epoch 8/30\n",
            "2000/2000 [==============================] - 213s 106ms/step - loss: 2.4173\n",
            "Epoch 9/30\n",
            "2000/2000 [==============================] - 216s 108ms/step - loss: 2.3820\n",
            "Epoch 10/30\n",
            "2000/2000 [==============================] - 217s 108ms/step - loss: 2.3476\n",
            "Epoch 11/30\n",
            "2000/2000 [==============================] - 218s 109ms/step - loss: 2.3208\n",
            "Epoch 12/30\n",
            "1020/2000 [==============>...............] - ETA: 1:46 - loss: 2.2889"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "metadata": {
        "id": "I5BtWqyErSxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_predictions(image, beam_index = 3):\n",
        "    start = [wordtoix[\"startseq\"]]\n",
        "    start_word = [[start, 0.0]]\n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "            preds = model.predict([image,par_caps], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            # Getting the top <beam_index>(n) predictions and creating a \n",
        "            # new list so as to put them via the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        start_word = start_word[-beam_index:]\n",
        "    \n",
        "    start_word = start_word[-1][0]\n",
        "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
        "    final_caption = []\n",
        "    \n",
        "    for i in intermediate_caption:\n",
        "        if i != 'endseq':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ],
      "metadata": {
        "id": "wlEDoELlrWU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tKVuxRCirbID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}