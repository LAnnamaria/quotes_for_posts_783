{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You know you're in love when you can't fall as...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote             author  \\\n",
       "0  I'm selfish, impatient and a little insecure. ...     Marilyn Monroe   \n",
       "1  You've gotta dance like there's nobody watchin...  William W. Purkey   \n",
       "2  You know you're in love when you can't fall as...          Dr. Seuss   \n",
       "\n",
       "                                            category  \n",
       "0  attributed-no-source, best, life, love, mistak...  \n",
       "1  dance, heaven, hurt, inspirational, life, love...  \n",
       "2  attributed-no-source, dreams, love, reality, s...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"../raw_data/quotes - reduced.csv\"\n",
    "quotes = pd.read_csv(file, decimal=\",\")\n",
    "quotes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156656, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I'm selfish, impatient and a little insecure. ...\n",
       "1         You've gotta dance like there's nobody watchin...\n",
       "2         You know you're in love when you can't fall as...\n",
       "3         A friend is someone who knows all about you an...\n",
       "4         Darkness cannot drive out darkness: only light...\n",
       "                                ...                        \n",
       "156651    The harassed look is that of a desperately tir...\n",
       "156652    â€¦In this way that he sought to control the ver...\n",
       "156653    No matter how we choose to live, we both die a...\n",
       "156654    The goal that you hope you will one day arrive...\n",
       "156655    I've spent years living safely to secure a lon...\n",
       "Name: quote, Length: 156656, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[\"quote\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['quote'] = quotes[\"quote\"].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all quotes\n",
    "quotes['clean_quotes'] = quotes.quote.apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes[\"clean_quotes\"] = quotes[\"clean_quotes\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['selfish', 'impatient', 'little', 'insecure',...\n",
       "1         ['got', 'ta', 'dance', 'like', 'nobody', 'watc...\n",
       "2         ['know', 'love', 'fall', 'asleep', 'reality', ...\n",
       "3            ['friend', 'someone', 'know', 'still', 'love']\n",
       "4         ['darkness', 'drive', 'darkness', 'light', 'ha...\n",
       "                                ...                        \n",
       "156651    ['harassed', 'look', 'desperately', 'tired', '...\n",
       "156652    ['way', 'sought', 'control', 'passage', 'life'...\n",
       "156653           ['matter', 'choose', 'live', 'die', 'end']\n",
       "156654    ['goal', 'hope', 'one', 'day', 'arrive', 'long...\n",
       "156655    ['spent', 'year', 'living', 'safely', 'secure'...\n",
       "Name: clean_quotes, Length: 156656, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[\"clean_quotes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train an LDA model to extract potential topics.\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(quotes['clean_quotes'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5)\n",
    "\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.59326327, 0.34699743, 0.20003217, ..., 0.20000486, 1.19712089,\n",
       "        1.68206954],\n",
       "       [0.21407794, 1.99162189, 1.19897809, ..., 0.20483451, 0.20485768,\n",
       "        0.20384361],\n",
       "       [5.7849797 , 0.20000675, 1.17963265, ..., 0.20000273, 0.20703014,\n",
       "        0.20395634],\n",
       "       [0.203582  , 0.20000668, 0.2047127 , ..., 2.19515438, 1.19066664,\n",
       "        1.7093748 ],\n",
       "       [0.20409709, 0.26136724, 0.21664439, ..., 0.20000352, 0.20032465,\n",
       "        0.20075571]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('woman', 6991.357279013261), ('said', 3647.1041550022883), ('like', 3038.973763734751), ('men', 2984.1012339801123), ('child', 2310.7438039529115), ('man', 2073.4589737791985), ('sex', 1854.4062632514404), ('year', 1540.188378276173), ('girl', 1500.8982335913943), ('mother', 1398.6305062257554)]\n",
      "Topic 1:\n",
      "[('life', 11247.243609455589), ('people', 2977.664212865179), ('happiness', 2776.1126657758114), ('mind', 2715.010856449198), ('time', 2654.0115951983703), ('great', 2400.455789573284), ('make', 2388.05899421583), ('work', 2333.671322852402), ('war', 2217.676207473787), ('success', 2216.191595933482)]\n",
      "Topic 2:\n",
      "[('life', 11048.858126884956), ('thing', 10315.308848430703), ('time', 10100.664794006188), ('people', 9953.869019604164), ('know', 9940.737753334213), ('want', 9125.284671022135), ('like', 7962.043647068456), ('make', 7063.428218425889), ('think', 7050.802070918503), ('book', 5806.13743735092)]\n",
      "Topic 3:\n",
      "[('love', 14380.654440410828), ('god', 13663.864864538524), ('world', 5500.558717104596), ('man', 5309.831819898145), ('human', 4702.715970107133), ('people', 4391.41145681689), ('life', 3911.149803041035), ('power', 3817.9056912705323), ('truth', 3712.479998436676), ('thing', 3673.2365999015387)]\n",
      "Topic 4:\n",
      "[('like', 7184.232886466831), ('eye', 4462.489059083279), ('heart', 3822.799706108068), ('love', 3590.288121717282), ('light', 3244.3013651333804), ('world', 3139.973233701303), ('hand', 2621.11813078175), ('said', 2057.2790943604728), ('soul', 1942.5118067139192), ('night', 1915.3337536925426)]\n"
     ]
    }
   ],
   "source": [
    "#function to print potential topics\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : 0.06676896484288439\n",
      "topic 1 : 0.06667520503362068\n",
      "topic 2 : 0.7326275579747601\n",
      "topic 3 : 0.06681882686100665\n",
      "topic 4 : 0.06710944528772816\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "example =[\"I am so sad and I want to cry.\"]\n",
    "\n",
    "example_vectorized = vectorizer.transform(example)\n",
    "\n",
    "lda_vectors_cry = lda_model.transform(example_vectorized)\n",
    "\n",
    "print(\"topic 0 :\", lda_vectors_cry[0][0])\n",
    "print(\"topic 1 :\", lda_vectors_cry[0][1])\n",
    "print(\"topic 2 :\", lda_vectors_cry[0][2])\n",
    "print(\"topic 3 :\", lda_vectors_cry[0][3])\n",
    "print(\"topic 4 :\", lda_vectors_cry[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_topic(text):\n",
    "    #text=[str(text)]\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "    ev=vectorizer.transform(text)\n",
    "    lda_ev=lda_model.transform(ev)\n",
    "    topic_score={}\n",
    "    for i in range(5):\n",
    "        topic_score.update({f\"topic{i}\": lda_ev[0][i]})\n",
    "    return topic_score\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am so sad and I want to cry.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbest_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mbest_topic\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_topic\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#text=[str(text)]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     ev\u001b[38;5;241m=\u001b[39m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     lda_ev\u001b[38;5;241m=\u001b[39mlda_model\u001b[38;5;241m.\u001b[39mtransform(ev)\n\u001b[1;32m      6\u001b[0m     topic_score\u001b[38;5;241m=\u001b[39m{}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/quotes_for_posts_783/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m     )\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/quotes_for_posts_783/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:498\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[0;32m--> 498\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "text =[\"I am so sad and I want to cry.\"]\n",
    "best_topic(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Tuning: Bag of words & TfidfVectorizer & MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'tfidf__min_df': (0.05,0.1),\n",
    "    'tfidf__max_df': (0.75,1),\n",
    "    'nb__alpha': (0.01,0.1,1,10),}\n",
    "\n",
    "# Perform grid search on pipeline\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(data.clean_reviews,data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Bag of words for top 100 features\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",max_features=100)\n",
    "\n",
    "X1 = vectorizer.fit_transform(quotes[\"clean_quotes\"])\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer for 100 features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 100,stop_words=\"english\")\n",
    "\n",
    "X2 = tf_idf_vectorizer.fit_transform(quotes[\"clean_quotes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer for 100 features of combination words\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 100,ngram_range=(2,2))\n",
    "\n",
    "X3 = tf_idf_vectorizer.fit_transform(quotes[\"clean_quotes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
