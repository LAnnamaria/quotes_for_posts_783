{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You know you're in love when you can't fall as...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote             author  \\\n",
       "0  I'm selfish, impatient and a little insecure. ...     Marilyn Monroe   \n",
       "1  You've gotta dance like there's nobody watchin...  William W. Purkey   \n",
       "2  You know you're in love when you can't fall as...          Dr. Seuss   \n",
       "\n",
       "                                            category  \n",
       "0  attributed-no-source, best, life, love, mistak...  \n",
       "1  dance, heaven, hurt, inspirational, life, love...  \n",
       "2  attributed-no-source, dreams, love, reality, s...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"../raw_data/quotes.csv\"\n",
    "quotes = pd.read_csv(file, decimal=\",\")\n",
    "quotes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499709, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I'm selfish, impatient and a little insecure. ...\n",
       "1         You've gotta dance like there's nobody watchin...\n",
       "2         You know you're in love when you can't fall as...\n",
       "3         A friend is someone who knows all about you an...\n",
       "4         Darkness cannot drive out darkness: only light...\n",
       "                                ...                        \n",
       "499704    I do believe the most important thing I can do...\n",
       "499705    I'd say I'm a bit antimadridista although I do...\n",
       "499706                                   The future is now.\n",
       "499707    In all my life and in the future, I will alway...\n",
       "499708      The future is as bright as the promises of God.\n",
       "Name: quote, Length: 499709, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[\"quote\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['quote'] = quotes[\"quote\"].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all quotes\n",
    "quotes['clean_quotes'] = quotes.quote.apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes[\"clean_quotes\"] = quotes[\"clean_quotes\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['selfish', 'impatient', 'little', 'insecure',...\n",
       "1         ['got', 'ta', 'dance', 'like', 'nobody', 'watc...\n",
       "2         ['know', 'love', 'fall', 'asleep', 'reality', ...\n",
       "3            ['friend', 'someone', 'know', 'still', 'love']\n",
       "4         ['darkness', 'drive', 'darkness', 'light', 'ha...\n",
       "                                ...                        \n",
       "499704    ['believe', 'important', 'thing', 'help', 'you...\n",
       "499705    ['say', 'bit', 'antimadridista', 'although', '...\n",
       "499706                                           ['future']\n",
       "499707    ['life', 'future', 'always', 'faithful', 'loya...\n",
       "499708               ['future', 'bright', 'promise', 'god']\n",
       "Name: clean_quotes, Length: 499709, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[\"clean_quotes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train an LDA model to extract potential topics.\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(quotes['clean_quotes'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5)\n",
    "\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.84701682,  6.39591812,  0.50194978, ...,  0.50403234,\n",
       "         0.50317524,  0.52154638],\n",
       "       [10.15298318,  0.60408188,  1.49805022, ...,  1.49596766,\n",
       "         1.49682475,  2.47845362]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/.pyenv/versions/3.8.12/envs/quotes_for_posts_783/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 29882.191088209598), ('know', 22798.17746946296), ('said', 22061.22797446991), ('get', 18451.050082623293), ('would', 17262.68789726628), ('want', 16627.84613922117), ('one', 14612.866313438763), ('say', 14486.818064221916), ('think', 14447.881970374992), ('time', 13640.231956193033)]\n",
      "Topic 1:\n",
      "[('love', 52671.204452057944), ('life', 48549.03256128378), ('one', 28180.57874740819), ('thing', 25249.007702776722), ('god', 25029.04770806585), ('never', 16753.044346147068), ('make', 16632.29248432716), ('time', 15801.693745430932), ('know', 15622.73078421346), ('people', 14978.6522497944)]\n",
      "Topic 2:\n",
      "[('people', 29761.479071480728), ('one', 9836.047653654647), ('world', 8338.110937271962), ('make', 7757.670741665258), ('human', 7281.362039390888), ('think', 7266.280308760845), ('right', 6150.1592636973155), ('problem', 5866.5126777233245), ('society', 5836.689424590782), ('money', 5787.881581774315)]\n",
      "Topic 3:\n",
      "[('like', 21126.150138647325), ('light', 10054.661800125366), ('one', 9335.586243708307), ('eye', 8299.484158027764), ('world', 7690.566273704645), ('could', 7373.918396491939), ('would', 7182.869631784258), ('night', 6442.909079004504), ('see', 6296.446174092592), ('time', 5902.869335445232)]\n",
      "Topic 4:\n",
      "[('life', 19197.4726120826), ('one', 18027.921041788195), ('man', 15893.790121792399), ('work', 10321.028349620246), ('every', 10169.417278847577), ('time', 10048.240532300148), ('god', 9488.017164926636), ('truth', 9314.800473309724), ('word', 8732.146426076317), ('world', 8609.792916770602)]\n"
     ]
    }
   ],
   "source": [
    "#function to print potential topics\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : 0.645584306306476\n",
      "topic 1 : 0.03401577570857187\n",
      "topic 2 : 0.03344023144359796\n",
      "topic 3 : 0.2528053131374051\n",
      "topic 4 : 0.03415437340394907\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "example =[\"I am so sad and I want to cry.\"]\n",
    "\n",
    "example_vectorized = vectorizer.transform(example)\n",
    "\n",
    "lda_vectors = lda_model.transform(example_vectorized)\n",
    "\n",
    "print(\"topic 0 :\", lda_vectors[0][0])\n",
    "print(\"topic 1 :\", lda_vectors[0][1])\n",
    "print(\"topic 2 :\", lda_vectors[0][2])\n",
    "print(\"topic 3 :\", lda_vectors[0][3])\n",
    "print(\"topic 4 :\", lda_vectors[0][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Tuning: Bag of words & TfidfVectorizer & MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'tfidf__min_df': (0.05,0.1),\n",
    "    'tfidf__max_df': (0.75,1),\n",
    "    'nb__alpha': (0.01,0.1,1,10),}\n",
    "\n",
    "# Perform grid search on pipeline\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(data.clean_reviews,data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['away',\n",
       " 'bad',\n",
       " 'beautiful',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'body',\n",
       " 'book',\n",
       " 'change',\n",
       " 'child',\n",
       " 'come',\n",
       " 'day',\n",
       " 'death',\n",
       " 'different',\n",
       " 'dream',\n",
       " 'end',\n",
       " 'experience',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fear',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'friend',\n",
       " 'future',\n",
       " 'god',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'hand',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'head',\n",
       " 'heart',\n",
       " 'help',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'important',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'learn',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'make',\n",
       " 'man',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'men',\n",
       " 'mind',\n",
       " 'moment',\n",
       " 'nature',\n",
       " 'need',\n",
       " 'new',\n",
       " 'night',\n",
       " 'old',\n",
       " 'pain',\n",
       " 'past',\n",
       " 'people',\n",
       " 'person',\n",
       " 'place',\n",
       " 'power',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'right',\n",
       " 'said',\n",
       " 'say',\n",
       " 'self',\n",
       " 'sense',\n",
       " 'soul',\n",
       " 'story',\n",
       " 'tell',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'true',\n",
       " 'truth',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'way',\n",
       " 'woman',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of words for top 100 features\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",max_features=100)\n",
    "\n",
    "X1 = vectorizer.fit_transform(quotes[\"clean_quotes\"])\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer for 100 features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 100,stop_words=\"english\")\n",
    "\n",
    "X2 = tf_idf_vectorizer.fit_transform(quotes[\"clean_quotes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['away',\n",
       " 'bad',\n",
       " 'beautiful',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'body',\n",
       " 'book',\n",
       " 'change',\n",
       " 'child',\n",
       " 'come',\n",
       " 'day',\n",
       " 'death',\n",
       " 'different',\n",
       " 'dream',\n",
       " 'end',\n",
       " 'experience',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fear',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'friend',\n",
       " 'future',\n",
       " 'god',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'hand',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'head',\n",
       " 'heart',\n",
       " 'help',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'important',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'learn',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'make',\n",
       " 'man',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'men',\n",
       " 'mind',\n",
       " 'moment',\n",
       " 'nature',\n",
       " 'need',\n",
       " 'new',\n",
       " 'night',\n",
       " 'old',\n",
       " 'pain',\n",
       " 'past',\n",
       " 'people',\n",
       " 'person',\n",
       " 'place',\n",
       " 'power',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'right',\n",
       " 'said',\n",
       " 'say',\n",
       " 'self',\n",
       " 'sense',\n",
       " 'soul',\n",
       " 'story',\n",
       " 'tell',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'true',\n",
       " 'truth',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'way',\n",
       " 'woman',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer for 100 features of combination words\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 100,ngram_range=(2,2))\n",
    "\n",
    "X3 = tf_idf_vectorizer.fit_transform(quotes[\"clean_quotes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another person',\n",
       " 'anyone else',\n",
       " 'anything else',\n",
       " 'best friend',\n",
       " 'best way',\n",
       " 'come back',\n",
       " 'could never',\n",
       " 'could see',\n",
       " 'day day',\n",
       " 'even know',\n",
       " 'even though',\n",
       " 'every day',\n",
       " 'every man',\n",
       " 'every moment',\n",
       " 'every single',\n",
       " 'every time',\n",
       " 'everyone else',\n",
       " 'everything else',\n",
       " 'fall love',\n",
       " 'feel like',\n",
       " 'felt like',\n",
       " 'find way',\n",
       " 'first time',\n",
       " 'go back',\n",
       " 'god love',\n",
       " 'gon na',\n",
       " 'good bad',\n",
       " 'good thing',\n",
       " 'hard work',\n",
       " 'high school',\n",
       " 'human being',\n",
       " 'human life',\n",
       " 'important thing',\n",
       " 'jesus christ',\n",
       " 'know know',\n",
       " 'let go',\n",
       " 'life life',\n",
       " 'life like',\n",
       " 'life one',\n",
       " 'life without',\n",
       " 'little bit',\n",
       " 'live life',\n",
       " 'long time',\n",
       " 'look back',\n",
       " 'look like',\n",
       " 'lot people',\n",
       " 'love life',\n",
       " 'love love',\n",
       " 'make difference',\n",
       " 'make feel',\n",
       " 'make life',\n",
       " 'make sense',\n",
       " 'make sure',\n",
       " 'many people',\n",
       " 'many thing',\n",
       " 'many time',\n",
       " 'men woman',\n",
       " 'never know',\n",
       " 'new york',\n",
       " 'nothing else',\n",
       " 'one another',\n",
       " 'one day',\n",
       " 'one ever',\n",
       " 'one know',\n",
       " 'one life',\n",
       " 'one must',\n",
       " 'one one',\n",
       " 'one person',\n",
       " 'one thing',\n",
       " 'one way',\n",
       " 'people know',\n",
       " 'people like',\n",
       " 'people love',\n",
       " 'people say',\n",
       " 'people think',\n",
       " 'people want',\n",
       " 'read book',\n",
       " 'real life',\n",
       " 'really want',\n",
       " 'rest life',\n",
       " 'someone else',\n",
       " 'something else',\n",
       " 'take away',\n",
       " 'take care',\n",
       " 'take place',\n",
       " 'thing happen',\n",
       " 'thing life',\n",
       " 'thing like',\n",
       " 'time time',\n",
       " 'true love',\n",
       " 'united state',\n",
       " 'want go',\n",
       " 'want know',\n",
       " 'whole life',\n",
       " 'whole world',\n",
       " 'would like',\n",
       " 'would never',\n",
       " 'would say',\n",
       " 'year ago',\n",
       " 'year old']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
