{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You know you're in love when you can't fall as...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote             author  \\\n",
       "0  I'm selfish, impatient and a little insecure. ...     Marilyn Monroe   \n",
       "1  You've gotta dance like there's nobody watchin...  William W. Purkey   \n",
       "2  You know you're in love when you can't fall as...          Dr. Seuss   \n",
       "\n",
       "                                            category  \n",
       "0  attributed-no-source, best, life, love, mistak...  \n",
       "1  dance, heaven, hurt, inspirational, life, love...  \n",
       "2  attributed-no-source, dreams, love, reality, s...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"../raw_data/quotes - reduced.csv\"\n",
    "quotes = pd.read_csv(file, decimal=\",\")\n",
    "quotes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156656, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I'm selfish, impatient and a little insecure. ...\n",
       "1         You've gotta dance like there's nobody watchin...\n",
       "2         You know you're in love when you can't fall as...\n",
       "3         A friend is someone who knows all about you an...\n",
       "4         Darkness cannot drive out darkness: only light...\n",
       "                                ...                        \n",
       "156651    The harassed look is that of a desperately tir...\n",
       "156652    â€¦In this way that he sought to control the ver...\n",
       "156653    No matter how we choose to live, we both die a...\n",
       "156654    The goal that you hope you will one day arrive...\n",
       "156655    I've spent years living safely to secure a lon...\n",
       "Name: quote, Length: 156656, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[\"quote\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['quote'] = quotes[\"quote\"].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all quotes\n",
    "quotes['clean_quotes'] = quotes.quote.apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes[\"clean_quotes\"] = quotes[\"clean_quotes\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['selfish', 'impatient', 'little', 'insecure',...\n",
       "1         ['got', 'ta', 'dance', 'like', 'nobody', 'watc...\n",
       "2         ['know', 'love', 'fall', 'asleep', 'reality', ...\n",
       "3            ['friend', 'someone', 'know', 'still', 'love']\n",
       "4         ['darkness', 'drive', 'darkness', 'light', 'ha...\n",
       "                                ...                        \n",
       "156651    ['harassed', 'look', 'desperately', 'tired', '...\n",
       "156652    ['way', 'sought', 'control', 'passage', 'life'...\n",
       "156653           ['matter', 'choose', 'live', 'die', 'end']\n",
       "156654    ['goal', 'hope', 'one', 'day', 'arrive', 'long...\n",
       "156655    ['spent', 'year', 'living', 'safely', 'secure'...\n",
       "Name: clean_quotes, Length: 156656, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[\"clean_quotes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train an LDA model to extract potential topics.\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(quotes['clean_quotes'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5)\n",
    "\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.26934913, 0.2000015 , 0.2000144 , ..., 0.20000464, 0.38207064,\n",
       "        2.1652035 ],\n",
       "       [0.20226623, 0.20000299, 1.2002096 , ..., 0.21003663, 0.20167896,\n",
       "        0.20183587],\n",
       "       [0.20221164, 0.20000115, 0.20500404, ..., 2.18995069, 0.20447902,\n",
       "        0.20349697],\n",
       "       [0.2036303 , 2.19567188, 0.20875525, ..., 0.20000498, 0.20000436,\n",
       "        0.20044245],\n",
       "       [6.1225427 , 0.20432248, 1.18601671, ..., 0.20000306, 2.01176702,\n",
       "        1.2290212 ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('love', 6692.1484566651325), ('human', 4085.890593548118), ('woman', 3885.952276808731), ('child', 3506.8041703140343), ('power', 3374.864407323377), ('life', 3044.1699906290546), ('world', 2780.727443276623), ('men', 2468.782294241673), ('art', 2457.034690325855), ('society', 2416.997675195514)]\n",
      "Topic 1:\n",
      "[('god', 13376.75914986686), ('people', 3420.3867553278264), ('faith', 2883.3438170919126), ('soul', 2357.464295337891), ('fear', 2295.285242775864), ('good', 2231.5647285853893), ('spirit', 1555.790922796209), ('religion', 1541.5497796740992), ('man', 1466.950104143869), ('strength', 1464.8806100127663)]\n",
      "Topic 2:\n",
      "[('life', 21597.160154022604), ('thing', 11796.939994352055), ('people', 9950.381817744257), ('make', 8792.93519996877), ('know', 8033.253921103877), ('time', 7815.436633249509), ('love', 7256.0782651871195), ('world', 6998.12078374794), ('want', 5699.5193901754465), ('think', 5280.486228225768)]\n",
      "Topic 3:\n",
      "[('world', 4542.957175153552), ('heart', 4181.404732702302), ('light', 3611.086940649239), ('love', 3193.612847155541), ('like', 3167.0398417284123), ('eye', 2834.134593746522), ('beauty', 2825.585920824082), ('soul', 2255.2126211983573), ('man', 1850.944202903386), ('earth', 1804.778003152791)]\n",
      "Topic 4:\n",
      "[('like', 12449.400323604394), ('said', 7452.684612955784), ('time', 5509.906711354721), ('know', 5388.02676866966), ('love', 4708.619293238595), ('book', 4503.6701849712135), ('say', 4338.1070710292315), ('want', 4241.547840527079), ('day', 3974.260541422576), ('think', 3821.0316825596747)]\n"
     ]
    }
   ],
   "source": [
    "#function to print potential topics\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : 0.06678087413720929\n",
      "topic 1 : 0.06670819622840185\n",
      "topic 2 : 0.7301792370785184\n",
      "topic 3 : 0.06712943467643834\n",
      "topic 4 : 0.06920225787943218\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "example =[\"I am so sad and I want to cry.\"]\n",
    "\n",
    "example_vectorized = vectorizer.transform(example)\n",
    "\n",
    "lda_vectors_cry = lda_model.transform(example_vectorized)\n",
    "\n",
    "print(\"topic 0 :\", lda_vectors_cry[0][0])\n",
    "print(\"topic 1 :\", lda_vectors_cry[0][1])\n",
    "print(\"topic 2 :\", lda_vectors_cry[0][2])\n",
    "print(\"topic 3 :\", lda_vectors_cry[0][3])\n",
    "print(\"topic 4 :\", lda_vectors_cry[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to returning proper topic for text\n",
    "def best_topic(text):\n",
    "    text=[text]\n",
    "    ev=vectorizer.transform(text)\n",
    "    lda_ev=lda_model.transform(ev)\n",
    "    topic_score={}\n",
    "    for i in range(5):\n",
    "        topic_score.update({f\"topic{i}\": lda_ev[0][i]})\n",
    "        a=max(topic_score.values())\n",
    "    for key, value in topic_score.items():\n",
    "        if a == value:\n",
    "            name=(key,a)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic2', 0.7301792370785184)\n"
     ]
    }
   ],
   "source": [
    "text =\"I am so sad and I want to cry\"\n",
    "best_topic(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic3', 0.4474368543715951)\n"
     ]
    }
   ],
   "source": [
    "text='I am so happy, I just want to dance and sing.'\n",
    "best_topic(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic3', 0.7982270809755947)\n"
     ]
    }
   ],
   "source": [
    "text='Here you can see my house next to the Brandenburg Gate in Berlin.'\n",
    "best_topic(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Tuning: Bag of words & TfidfVectorizer & MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'tfidf__min_df': (0.05,0.1),\n",
    "    'tfidf__max_df': (0.75,1),\n",
    "    'nb__alpha': (0.01,0.1,1,10),}\n",
    "\n",
    "# Perform grid search on pipeline\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(data.clean_reviews,data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/.pyenv/versions/3.8.12/envs/quotes_for_posts_783/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'away',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'body',\n",
       " 'book',\n",
       " 'change',\n",
       " 'child',\n",
       " 'come',\n",
       " 'day',\n",
       " 'death',\n",
       " 'different',\n",
       " 'dream',\n",
       " 'earth',\n",
       " 'end',\n",
       " 'experience',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'faith',\n",
       " 'fear',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'free',\n",
       " 'friend',\n",
       " 'god',\n",
       " 'going',\n",
       " 'good',\n",
       " 'great',\n",
       " 'hand',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'heart',\n",
       " 'hope',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'joy',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'learn',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'long',\n",
       " 'look',\n",
       " 'love',\n",
       " 'make',\n",
       " 'man',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'men',\n",
       " 'mind',\n",
       " 'moment',\n",
       " 'nature',\n",
       " 'need',\n",
       " 'new',\n",
       " 'night',\n",
       " 'old',\n",
       " 'pain',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'person',\n",
       " 'place',\n",
       " 'power',\n",
       " 'read',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'right',\n",
       " 'said',\n",
       " 'say',\n",
       " 'self',\n",
       " 'soul',\n",
       " 'story',\n",
       " 'tell',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'true',\n",
       " 'truth',\n",
       " 'understand',\n",
       " 'want',\n",
       " 'war',\n",
       " 'way',\n",
       " 'woman',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of words for top 100 features\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",max_features=100)\n",
    "\n",
    "X1 = vectorizer.fit_transform(quotes[\"clean_quotes\"])\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer for 100 features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 100,stop_words=\"english\")\n",
    "\n",
    "X2 = tf_idf_vectorizer.fit_transform(quotes[\"clean_quotes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'away',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'body',\n",
       " 'book',\n",
       " 'change',\n",
       " 'child',\n",
       " 'come',\n",
       " 'day',\n",
       " 'death',\n",
       " 'different',\n",
       " 'dream',\n",
       " 'earth',\n",
       " 'end',\n",
       " 'experience',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'faith',\n",
       " 'fear',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'free',\n",
       " 'friend',\n",
       " 'god',\n",
       " 'going',\n",
       " 'good',\n",
       " 'great',\n",
       " 'hand',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'heart',\n",
       " 'hope',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'joy',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'learn',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'long',\n",
       " 'look',\n",
       " 'love',\n",
       " 'make',\n",
       " 'man',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'men',\n",
       " 'mind',\n",
       " 'moment',\n",
       " 'nature',\n",
       " 'need',\n",
       " 'new',\n",
       " 'night',\n",
       " 'old',\n",
       " 'pain',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'person',\n",
       " 'place',\n",
       " 'power',\n",
       " 'read',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'right',\n",
       " 'said',\n",
       " 'say',\n",
       " 'self',\n",
       " 'soul',\n",
       " 'story',\n",
       " 'tell',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'true',\n",
       " 'truth',\n",
       " 'understand',\n",
       " 'want',\n",
       " 'war',\n",
       " 'way',\n",
       " 'woman',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer for 100 features of combination words\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 100,ngram_range=(2,2))\n",
    "\n",
    "X3 = tf_idf_vectorizer.fit_transform(quotes[\"clean_quotes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another person',\n",
       " 'anyone else',\n",
       " 'anything else',\n",
       " 'best friend',\n",
       " 'best way',\n",
       " 'change life',\n",
       " 'change world',\n",
       " 'come back',\n",
       " 'could never',\n",
       " 'could see',\n",
       " 'day day',\n",
       " 'even though',\n",
       " 'every day',\n",
       " 'every man',\n",
       " 'every moment',\n",
       " 'every single',\n",
       " 'every time',\n",
       " 'everyone else',\n",
       " 'everything else',\n",
       " 'fall love',\n",
       " 'feel like',\n",
       " 'felt like',\n",
       " 'find way',\n",
       " 'first time',\n",
       " 'go back',\n",
       " 'god love',\n",
       " 'gon na',\n",
       " 'good bad',\n",
       " 'good thing',\n",
       " 'hard work',\n",
       " 'human being',\n",
       " 'human life',\n",
       " 'important thing',\n",
       " 'jesus christ',\n",
       " 'know know',\n",
       " 'know love',\n",
       " 'let go',\n",
       " 'life life',\n",
       " 'life like',\n",
       " 'life love',\n",
       " 'life one',\n",
       " 'life without',\n",
       " 'live life',\n",
       " 'long time',\n",
       " 'look back',\n",
       " 'look like',\n",
       " 'love god',\n",
       " 'love life',\n",
       " 'love like',\n",
       " 'love love',\n",
       " 'love one',\n",
       " 'love someone',\n",
       " 'make feel',\n",
       " 'make life',\n",
       " 'make sense',\n",
       " 'make sure',\n",
       " 'many people',\n",
       " 'many thing',\n",
       " 'many time',\n",
       " 'men woman',\n",
       " 'never forget',\n",
       " 'never know',\n",
       " 'nothing else',\n",
       " 'one another',\n",
       " 'one day',\n",
       " 'one know',\n",
       " 'one life',\n",
       " 'one must',\n",
       " 'one one',\n",
       " 'one person',\n",
       " 'one thing',\n",
       " 'one way',\n",
       " 'people know',\n",
       " 'people like',\n",
       " 'people love',\n",
       " 'people say',\n",
       " 'people think',\n",
       " 'people want',\n",
       " 'read book',\n",
       " 'real life',\n",
       " 'really want',\n",
       " 'rest life',\n",
       " 'someone else',\n",
       " 'something else',\n",
       " 'take away',\n",
       " 'take care',\n",
       " 'take place',\n",
       " 'thing happen',\n",
       " 'thing life',\n",
       " 'thing world',\n",
       " 'thousand year',\n",
       " 'time time',\n",
       " 'two people',\n",
       " 'want know',\n",
       " 'whole life',\n",
       " 'whole world',\n",
       " 'would like',\n",
       " 'would never',\n",
       " 'year ago',\n",
       " 'year old']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
