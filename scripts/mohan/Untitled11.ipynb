{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsGQIbP_SkKW"
   },
   "source": [
    "#All installation needed for the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yDqsm1FRNvgS"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install TensorFlow\n",
    "!pip install Keras\n",
    "!pip install pillow\n",
    "!pip install NumPy\n",
    "!pip install tqdm\n",
    "!pip install time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPC6_HtFOLm7"
   },
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j4kTEBiATOSd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 05:52:47.667960: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:52:47.668240: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uxJ58tRTWiE",
    "outputId": "f11be50a-7aef-4c2b-962a-feb1f2cfaddd"
   },
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w83yLo51iidO",
    "outputId": "e4612aef-06be-47fa-e7a3-4a74eae52066"
   },
   "source": [
    "!mkdir raw_data\n",
    "!gcsfuse --implicit-dirs image_caption raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBrdON8hjCgN",
    "outputId": "833db626-c09f-44f6-c183-02128de90508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohanakrishnan/code/LAnnamaria/quotes_for_posts_783/scripts/mohan/raw_data\n"
     ]
    }
   ],
   "source": [
    "cd raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_flickr30k.json  \u001b[0m\u001b[01;34mflickr30k_images\u001b[0m/  glove.6B.300d.txt\r\n",
      "\u001b[01;34mFlicker8k_Dataset\u001b[0m/      \u001b[01;34mFlickr8k_text\u001b[0m/     \u001b[01;34mquotes\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkrcIJpRmd3w",
    "outputId": "6c16474e-969b-476e-ab3a-fbfd06d27056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
      "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n"
     ]
    }
   ],
   "source": [
    "token_path = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "train_images_path = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "images_path = 'Flicker8k_Dataset/'\n",
    "glove_path = 'glove.6B.300d.txt'\n",
    "\n",
    "doc = open(token_path,'r').read()\n",
    "print(doc[:410])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VU6bYcq-VWPd"
   },
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) > 2:\n",
    "          image_id = tokens[0].split('.')[0]\n",
    "          image_desc = ' '.join(tokens[1:])\n",
    "          if image_id not in descriptions:\n",
    "              descriptions[image_id] = list()\n",
    "          descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "85DPjIyLVZpA"
   },
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        desc = desc.split()\n",
    "        desc = [word.lower() for word in desc]\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIs5GSIVo_5F",
    "outputId": "8ddbc7a0-175e-464f-cfbd-f85da47a3ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 8828\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "        [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b-4aAfnupIWi"
   },
   "outputs": [],
   "source": [
    "lines = list()\n",
    "for key, desc_list in descriptions.items():\n",
    "    for desc in desc_list:\n",
    "        lines.append(key + ' ' + desc)\n",
    "new_descriptions = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ilw_vAu9pLAC"
   },
   "outputs": [],
   "source": [
    "doc = open(train_images_path,'r').read()\n",
    "dataset = list()\n",
    "for line in doc.split('\\n'):\n",
    "    if len(line) > 1:\n",
    "      identifier = line.split('.')[0]\n",
    "      dataset.append(identifier)\n",
    "\n",
    "train = set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6LqSB1KdpNmy"
   },
   "outputs": [],
   "source": [
    "img = glob.glob(images_path + '*.jpg')\n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "train_img = []\n",
    "for i in img: \n",
    "    if i[len(images_path):] in train_images:\n",
    "        train_img.append(i)\n",
    "\n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "test_img = []\n",
    "for i in img: \n",
    "    if i[len(images_path):] in test_images: \n",
    "        test_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "A8V2PDO9pP5K"
   },
   "outputs": [],
   "source": [
    "train_descriptions = dict()\n",
    "for line in new_descriptions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    if image_id in train:\n",
    "        if image_id not in train_descriptions:\n",
    "            train_descriptions[image_id] = list()\n",
    "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        train_descriptions[image_id].append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B9V5xPJLpTln"
   },
   "outputs": [],
   "source": [
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0xGhUuYpofw",
    "outputId": "f5202f3b-b4b8-462a-febb-5ce9e4876dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary = 1659\n"
     ]
    }
   ],
   "source": [
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "print('Vocabulary = %d' % (len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JAZyMqBSpZ0b"
   },
   "outputs": [],
   "source": [
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "vocab_size = len(ixtoword) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78HPGtRJpgkD",
    "outputId": "91d7aea2-9311-41ad-d52b-d9b927f5283c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 38\n"
     ]
    }
   ],
   "source": [
    "all_desc = list()\n",
    "for key in train_descriptions.keys():\n",
    "    [all_desc.append(d) for d in train_descriptions[key]]\n",
    "lines = all_desc\n",
    "max_length = max(len(d.split()) for d in lines)\n",
    "\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ixi4VioOpt-y"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {} \n",
    "f = open(os.path.join(glove_path), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rt8ON4eKQL2W"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xzNPVqdWpw1J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 05:58:40.120225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-07 05:58:40.120774: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.120851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.120910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.120965: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.121020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.121075: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.121128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.121182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-07 05:58:40.121191: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-07 05:58:40.131914: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 8s 0us/step\n",
      "96124928/96112376 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aBlSVCx9qojR"
   },
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WXVjVk38qrmi"
   },
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image) \n",
    "    fea_vec = model_new.predict(image) \n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
    "    return fea_vec\n",
    "\n",
    "encoding_train = {}\n",
    "for img in train_img:\n",
    "    encoding_train[img[len(images_path):]] = encode(img)\n",
    "train_features = encoding_train\n",
    "\n",
    "encoding_test = {}\n",
    "for img in test_img:\n",
    "    encoding_test[img[len(images_path):]] = encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nL12pY7zqvm9",
    "outputId": "c7da6a72-5348-4041-937d-62db72154a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 38, 300)      498000      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 38, 300)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          570368      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1660)         426620      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,085,324\n",
      "Trainable params: 2,085,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RNwroIKdq5uj"
   },
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_YN0eF4HrKkJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            if n==num_photos_per_batch:\n",
    "                yield ([array(X1), array(X2)], array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ev0rBbprrORx",
    "outputId": "1e83e160-0308-4822-b265-6b4c83d6ac65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 676s 336ms/step - loss: 3.6445\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 683s 341ms/step - loss: 2.9825\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 676s 338ms/step - loss: 2.7896\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 681s 340ms/step - loss: 2.6719\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 681s 340ms/step - loss: 2.5871\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 678s 339ms/step - loss: 2.5225\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 681s 341ms/step - loss: 2.4689\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 673s 337ms/step - loss: 2.4279\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 679s 339ms/step - loss: 2.3897\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 681s 341ms/step - loss: 2.3591\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 676s 338ms/step - loss: 2.3329\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 683s 341ms/step - loss: 2.3105\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 678s 339ms/step - loss: 2.2888\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 773s 387ms/step - loss: 2.2698\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 798s 399ms/step - loss: 2.2537\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 914s 457ms/step - loss: 2.2362\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 874s 437ms/step - loss: 2.2246\n",
      "Epoch 18/30\n",
      "1860/2000 [==========================>...] - ETA: 1:10 - loss: 2.2096"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 3\n",
    "steps = len(train_descriptions)//batch_size\n",
    "\n",
    "generator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\n",
    "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5BtWqyErSxz"
   },
   "outputs": [],
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlEDoELlrWU0"
   },
   "outputs": [],
   "source": [
    "def beam_search_predictions(image, beam_index = 3):\n",
    "    start = [wordtoix[\"startseq\"]]\n",
    "    start_word = [[start, 0.0]]\n",
    "    while len(start_word[0][0]) < max_length:\n",
    "        temp = []\n",
    "        for s in start_word:\n",
    "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
    "            preds = model.predict([image,par_caps], verbose=0)\n",
    "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
    "            # Getting the top <beam_index>(n) predictions and creating a \n",
    "            # new list so as to put them via the model again\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "                prob += preds[0][w]\n",
    "                temp.append([next_cap, prob])\n",
    "                    \n",
    "        start_word = temp\n",
    "        # Sorting according to the probabilities\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        # Getting the top words\n",
    "        start_word = start_word[-beam_index:]\n",
    "    \n",
    "    start_word = start_word[-1][0]\n",
    "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
    "    final_caption = []\n",
    "    \n",
    "    for i in intermediate_caption:\n",
    "        if i != 'endseq':\n",
    "            final_caption.append(i)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKVuxRCirbID"
   },
   "outputs": [],
   "source": [
    "pic = '2398605966_1d0c9e6a20.jpg'\n",
    "image = encoding_test[pic].reshape((1,2048))\n",
    "x=plt.imread(images_path+pic)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "print(\"Greedy Search:\",greedySearch(image))\n",
    "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
    "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
    "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
    "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
